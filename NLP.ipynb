{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SumaSrija/NLP/blob/main/NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyJGIzEOxgD3",
        "outputId": "d63c709c-87e8-4285-8967-b85c9b448a6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['Maximum', 'students', 'are', 'Suffering', 'from', 'Insominia', '.']\n",
            "Stemmed Tokens: ['maximum', 'student', 'are', 'suffer', 'from', 'insominia', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "def tokenize(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "def stem(tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "text = \"Maximum students are Suffering from Insominia.\"\n",
        "tokens = tokenize(text)\n",
        "print(\"Tokens:\", tokens)\n",
        "stemmed_tokens = stem(tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84Navv7yxyvS",
        "outputId": "9576fdcd-b73b-45a7-8fb6-0574d6e59e02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "There is a change in a medium\n",
            "\n",
            "Text after removing connectors and prepositions:\n",
            "There change medium\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "def remove_connectors_and_prepositions(text):\n",
        "    # Tokenize the text into words\n",
        "    words = word_tokenize(text)\n",
        "    # Get the English stop words (connectors and prepositions)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    # Remove stop words from the list of words\n",
        "    filtered_words = [word for word in words if word not in stop_words]\n",
        "    # Reconstruct the text without connectors and prepositions\n",
        "    filtered_text = ' '.join(filtered_words)\n",
        "    return filtered_text\n",
        "#text = \"This is a sample sentence with connectors and prepositions. However, we want to remove them from the text.\"\n",
        "text = \"There is a change in a medium\"\n",
        "# Remove connectors and prepositions\n",
        "result_text = remove_connectors_and_prepositions(text)\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nText after removing connectors and prepositions:\")\n",
        "print(result_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPQflp2pv9N0"
      },
      "source": [
        "ex2_lab\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk--KUXlwCNY",
        "outputId": "625e4e4a-dfbb-47ae-d524-60ff0b299413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chinnu: PROPN\n",
            "is: AUX\n",
            "the: DET\n",
            "one: NUM\n",
            "who: PRON\n",
            "protect: VERB\n",
            "lilly: ADV\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Your input text\n",
        "text= \"Chinnu is the one who protect lilly.\"\n",
        "#text = \"Goveínment of India is taking all necessaíy steps to ensuíe that we aíe píepaíed well to face the challenge and thíeat posed by the gíowing pandemic of COVID-19 the Coíona Viíus.\"\n",
        "\n",
        "# Process the text using spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print the token and its part of speech\n",
        "for token in doc:\n",
        "    print(f\"{token.text}: {token.pos_}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import brown\n",
        "\n",
        "# Load the Brown Corpus\n",
        "nltk.download('brown')\n",
        "brown_corpus = brown.words()\n",
        "\n",
        "# Filter words by genre\n",
        "fiction_words = brown.words(categories='fiction')\n",
        "lore_words = brown.words(categories='lore')\n",
        "\n",
        "# Create Conditional Frequency Distribution\n",
        "cfd = nltk.ConditionalFreqDist(\n",
        "    (genre, word.lower())\n",
        "    for genre in ['fiction', 'lore']\n",
        "    for word in brown.words(categories=genre)\n",
        ")\n",
        "\n",
        "# Print most common words in both genres\n",
        "print(\"Most common words in 'fiction' genre:\")\n",
        "print(cfd['fiction'].most_common(10))\n",
        "\n",
        "print(\"\\nMost common words in 'lore' genre:\")\n",
        "print(cfd['lore'].most_common(10))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akqQ4SdPDUGj",
        "outputId": "d4e3ad54-8c46-4ad0-d144-4e030c44a3f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most common words in 'fiction' genre:\n",
            "[('the', 3792), (',', 3654), ('.', 3639), ('and', 1770), ('to', 1508), ('of', 1423), ('a', 1339), ('he', 1308), ('was', 1091), ('in', 971)]\n",
            "\n",
            "Most common words in 'lore' genre:\n",
            "[('the', 6975), (',', 5519), ('.', 4367), ('of', 3696), ('and', 2833), ('to', 2581), ('a', 2426), ('in', 2196), ('is', 1016), ('that', 1005)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import udhr\n",
        "from collections import defaultdict\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('udhr')\n",
        "\n",
        "# Define conditions and frequencies to check\n",
        "conditions = ['Chickasaw', 'English', 'German_Deutsch']\n",
        "frequencies = [2, 4, 7, 9]\n",
        "\n",
        "# Initialize a defaultdict to store results\n",
        "results = defaultdict(dict)\n",
        "\n",
        "# Process each language condition\n",
        "for condition in conditions:\n",
        "    # Filter words by language condition\n",
        "    words = udhr.words(condition + '-Latin1')\n",
        "\n",
        "    # Calculate frequency distribution\n",
        "    freq_dist = nltk.FreqDist(words)\n",
        "\n",
        "    # Count words with specified frequencies\n",
        "    for freq in frequencies:\n",
        "        count = sum(1 for _, f in freq_dist.items() if f == freq)\n",
        "        results[condition][freq] = count\n",
        "\n",
        "# Tabulate the results\n",
        "table_data = [[\"Language\"] + frequencies]\n",
        "for condition in conditions:\n",
        "    row = [condition] + [results[condition].get(freq, 0) for freq in frequencies]\n",
        "    table_data.append(row)\n",
        "\n",
        "print(tabulate(table_data, headers=\"firstrow\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuOXZhaiJEzk",
        "outputId": "7f867a44-f983-45d0-b417-b5dcbf7af912"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/udhr.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Language          2    4    7    9\n",
            "--------------  ---  ---  ---  ---\n",
            "Chickasaw        50   13    5    0\n",
            "English          76   17    2    6\n",
            "German_Deutsch   75   14    9    1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import genesis\n",
        "import random\n",
        "\n",
        "# Download necessary resources\n",
        "nltk.download('genesis')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Load the Genesis Corpus\n",
        "corpus = genesis.words('english-web.txt')\n",
        "\n",
        "# Create bigrams\n",
        "bigrams = list(nltk.bigrams(corpus))\n",
        "\n",
        "# Calculate Conditional Frequency Distribution\n",
        "cfd = nltk.ConditionalFreqDist(bigrams)\n",
        "\n",
        "# Function to generate a sentence with most likely occurring words\n",
        "def generate_sentence(seed_word, length):\n",
        "    sentence = [seed_word]\n",
        "    current_word = seed_word\n",
        "    for _ in range(length - 1):\n",
        "        next_word = cfd[current_word].max()\n",
        "        sentence.append(next_word)\n",
        "        current_word = next_word\n",
        "    return ' '.join(sentence)\n",
        "\n",
        "# Generate sentence with seed word \"darkness\" and length 25\n",
        "seed_word = \"darkness\"\n",
        "sentence_length = 25\n",
        "generated_sentence = generate_sentence(seed_word, sentence_length)\n",
        "\n",
        "# Print the generated sentence\n",
        "print(\"Generated Sentence:\")\n",
        "print(generated_sentence)\n"
      ],
      "metadata": {
        "id": "fnSVKnthLfXT",
        "outputId": "c24d996e-4d7f-4721-fd24-209ff121a1e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/genesis.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sentence:\n",
            "darkness . He said , and the land of the land of the land of the land of the land of the land of the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "week 8 In Lab\n"
      ],
      "metadata": {
        "id": "vUzcz6CyLT-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "    return filtered_sentence\n",
        "\n",
        "def identify_parts_of_speech(text):\n",
        "    words = word_tokenize(text)\n",
        "    tagged_words = pos_tag(words)\n",
        "    return tagged_words\n",
        "\n",
        "# Sample input\n",
        "paragraphs = [\n",
        "    \"gowtham, viswa and hari are my good friends.\",\n",
        "    \"viswa is getting married next year.\",\n",
        "    \"hari is the innocent among three. gowtham is intelligent among them.\"\n",
        "]\n",
        "\n",
        "# Processing each paragraph\n",
        "for paragraph in paragraphs:\n",
        "    filtered_sentence = remove_stopwords(paragraph)\n",
        "    tagged_words = identify_parts_of_speech(paragraph)\n",
        "    print(filtered_sentence)\n",
        "    print(tagged_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MN1xRmk1LXVj",
        "outputId": "140d4818-ec02-4c49-f0f9-203c6ad723bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['gowtham', ',', 'viswa', 'hari', 'good', 'friends', '.']\n",
            "[('gowtham', 'NN'), (',', ','), ('viswa', 'NN'), ('and', 'CC'), ('hari', 'NN'), ('are', 'VBP'), ('my', 'PRP$'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
            "['viswa', 'getting', 'married', 'next', 'year', '.']\n",
            "[('viswa', 'NN'), ('is', 'VBZ'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
            "['hari', 'innocent', 'among', 'three', '.', 'gowtham', 'intelligent', 'among', '.']\n",
            "[('hari', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('innocent', 'NN'), ('among', 'IN'), ('three', 'CD'), ('.', '.'), ('gowtham', 'NN'), ('is', 'VBZ'), ('intelligent', 'JJ'), ('among', 'IN'), ('them', 'PRP'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PostLab\n"
      ],
      "metadata": {
        "id": "eBlLm2YbLe-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove all non-word characters (keep only letters and spaces)\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def bag_of_words(text):\n",
        "    # Tokenize the preprocessed text\n",
        "    words = word_tokenize(text)\n",
        "    # Count the occurrences of each word\n",
        "    word_counts = Counter(words)\n",
        "    return word_counts\n",
        "\n",
        "# Sample input\n",
        "text = \"\"\"Beans. I was trying to explain to somebody as we were flying in, that’s corn. That’s beans. And they werevery impressed at my agricultural knowledge. Please give it up\n",
        "for Amaury once again for that outstandingintroduction. I have a bunch of good\n",
        "friends here today, including somebody who I served with, who is one of the finest\n",
        "senators in the country, and we’re lucky to have him, your Senator, Dick Durbin is\n",
        "here.I also noticed, by the way, former Governor Edgar here, who I haven’t seen in\n",
        "a long time, and somehowhe has not aged and I have. And it’s great to see you,\n",
        "Governor. I want to thank President Killeen and everybody at the U of I System for\n",
        "making it possible for me to be here today. And I am deeply honored at the Paul\n",
        "Douglas Award that is being given to me. He is somebody who set the path for so\n",
        "much outstanding public service here in Illinois.\"\"\"\n",
        "\n",
        "# Preprocess the text\n",
        "preprocessed_text = preprocess_text(text)\n",
        "# Create bag of words representation\n",
        "word_counts = bag_of_words(preprocessed_text)\n",
        "\n",
        "# Print the bag of words\n",
        "print(word_counts)\n"
      ],
      "metadata": {
        "id": "cco5atV0Lgqi",
        "outputId": "7ab12b5f-fcbd-4380-9f32-c3d94cd0f83c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'i': 8, 'to': 7, 'and': 7, 'the': 6, 'for': 5, 'in': 4, 'here': 4, 'who': 4, 'is': 4, 'somebody': 3, 'at': 3, 'have': 3, 'of': 3, 'beans': 2, 'were': 2, 'thats': 2, 'it': 2, 'that': 2, 'a': 2, 'today': 2, 'governor': 2, 'me': 2, 'was': 1, 'trying': 1, 'explain': 1, 'as': 1, 'we': 1, 'flying': 1, 'corn': 1, 'they': 1, 'werevery': 1, 'impressed': 1, 'my': 1, 'agricultural': 1, 'knowledge': 1, 'please': 1, 'give': 1, 'up': 1, 'amaury': 1, 'once': 1, 'again': 1, 'outstandingintroduction': 1, 'bunch': 1, 'good': 1, 'friends': 1, 'including': 1, 'served': 1, 'with': 1, 'one': 1, 'finest': 1, 'senators': 1, 'country': 1, 'lucky': 1, 'him': 1, 'your': 1, 'senator': 1, 'dick': 1, 'durbin': 1, 'herei': 1, 'also': 1, 'noticed': 1, 'by': 1, 'way': 1, 'former': 1, 'edgar': 1, 'havent': 1, 'seen': 1, 'long': 1, 'time': 1, 'somehowhe': 1, 'has': 1, 'not': 1, 'aged': 1, 'its': 1, 'great': 1, 'see': 1, 'you': 1, 'want': 1, 'thank': 1, 'president': 1, 'killeen': 1, 'everybody': 1, 'u': 1, 'system': 1, 'making': 1, 'possible': 1, 'be': 1, 'am': 1, 'deeply': 1, 'honored': 1, 'paul': 1, 'douglas': 1, 'award': 1, 'being': 1, 'given': 1, 'he': 1, 'set': 1, 'path': 1, 'so': 1, 'much': 1, 'outstanding': 1, 'public': 1, 'service': 1, 'illinois': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 9 In Lab\n"
      ],
      "metadata": {
        "id": "K-prKbkWOpDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# (a) Import WordNet and find Synsets for the word \"hello\"\n",
        "synsets = wordnet.synsets(\"hello\")\n",
        "\n",
        "# (b) Get the word in the 0th index Synset using lemmas\n",
        "word = synsets[0].lemmas()[0].name()\n",
        "\n",
        "# (c) Name, Definition, and Examples of the first Synset\n",
        "synset_name = synsets[0].name()\n",
        "definition = synsets[0].definition()\n",
        "examples = synsets[0].examples()\n",
        "\n",
        "# (d) Synonyms and Antonyms in the Synset\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for lemma in synsets[0].lemmas():\n",
        "    synonyms.append(lemma.name())\n",
        "    if lemma.antonyms():\n",
        "        antonyms.append(lemma.antonyms()[0].name())\n",
        "\n",
        "# (e) Hypernyms and Hyponyms in the Synset\n",
        "hypernyms = []\n",
        "hyponyms = []\n",
        "\n",
        "for hypernym in synsets[0].hypernyms():\n",
        "    hypernyms.append(hypernym.name())\n",
        "for hyponym in synsets[0].hyponyms():\n",
        "    hyponyms.append(hyponym.name())\n",
        "\n",
        "# Printing the results\n",
        "print(\"(b) Word in 0th index Synset:\", word)\n",
        "print(\"(c) Name:\", synset_name)\n",
        "print(\"    Definition:\", definition)\n",
        "print(\"    Examples:\", examples)\n",
        "print(\"(d) Synonyms:\", synonyms)\n",
        "print(\"    Antonyms:\", antonyms)\n",
        "print(\"(e) Hypernyms:\", hypernyms)\n",
        "print(\"    Hyponyms:\", hyponyms)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kq5aILvAOrDz",
        "outputId": "8dc6d6d3-496b-466b-e324-4b15b56ec7b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(b) Word in 0th index Synset: hello\n",
            "(c) Name: hello.n.01\n",
            "    Definition: an expression of greeting\n",
            "    Examples: ['every morning they exchanged polite hellos']\n",
            "(d) Synonyms: ['hello', 'hullo', 'hi', 'howdy', 'how-do-you-do']\n",
            "    Antonyms: []\n",
            "(e) Hypernyms: ['greeting.n.01']\n",
            "    Hyponyms: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In-Lab-2"
      ],
      "metadata": {
        "id": "BUOnMYPYQAEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Define the words\n",
        "word1 = \"car\"\n",
        "word2 = \"bar\"\n",
        "\n",
        "# Get synsets for each word\n",
        "synsets1 = wordnet.synsets(word1)\n",
        "synsets2 = wordnet.synsets(word2)\n",
        "\n",
        "# Calculate Path Similarity\n",
        "path_similarity_scores = []\n",
        "for synset1 in synsets1:\n",
        "    for synset2 in synsets2:\n",
        "        path_similarity = synset1.path_similarity(synset2)\n",
        "        if path_similarity is not None:\n",
        "            path_similarity_scores.append(path_similarity)\n",
        "\n",
        "if path_similarity_scores:\n",
        "    average_path_similarity = sum(path_similarity_scores) / len(path_similarity_scores)\n",
        "else:\n",
        "    average_path_similarity = None\n",
        "\n",
        "# Calculate Wu-Palmer Similarity\n",
        "wu_palmer_similarity_scores = []\n",
        "for synset1 in synsets1:\n",
        "    for synset2 in synsets2:\n",
        "        wu_palmer_similarity = synset1.wup_similarity(synset2)\n",
        "        if wu_palmer_similarity is not None:\n",
        "            wu_palmer_similarity_scores.append(wu_palmer_similarity)\n",
        "\n",
        "if wu_palmer_similarity_scores:\n",
        "    average_wu_palmer_similarity = sum(wu_palmer_similarity_scores) / len(wu_palmer_similarity_scores)\n",
        "else:\n",
        "    average_wu_palmer_similarity = None\n",
        "\n",
        "print(\"(a) Path Similarity between 'car' and 'bar':\", average_path_similarity)\n",
        "print(\"(b) Wu-Palmer Similarity between 'car' and 'bar':\", average_wu_palmer_similarity)\n"
      ],
      "metadata": {
        "id": "pp8UBz7ZQB4V",
        "outputId": "740a8112-4489-42e8-e1a8-87dbbda8a19e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(a) Path Similarity between 'car' and 'bar': 0.09007835738990666\n",
            "(b) Wu-Palmer Similarity between 'car' and 'bar': 0.36107002261733706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Week 10 IN-LAB"
      ],
      "metadata": {
        "id": "HxufSIPUXuwG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grammar rules\n",
        "grammar = {\n",
        "    'S': [['NP', 'VP']],\n",
        "    'NP': [['the', 'noun'], ['a', 'noun']],\n",
        "    'VP': [['verb', 'NP']],\n",
        "    'noun': ['telescope', 'book', 'student'],\n",
        "    'verb': ['reads', 'studies']\n",
        "}\n",
        "\n",
        "# Function for recursive descent parsing\n",
        "def parse(sentence, index, goal):\n",
        "    if index == len(sentence):\n",
        "        return False\n",
        "\n",
        "    if isinstance(grammar[goal], list):\n",
        "        for production in grammar[goal]:\n",
        "            success = True\n",
        "            i = index\n",
        "            for symbol in production:\n",
        "                if i < len(sentence) and (symbol == sentence[i] or symbol in grammar):\n",
        "                    i += 1\n",
        "                else:\n",
        "                    success = False\n",
        "                    break\n",
        "            if success:\n",
        "                return i\n",
        "    elif goal == sentence[index]:\n",
        "        return index + 1\n",
        "\n",
        "    return False\n",
        "\n",
        "# Main parsing function\n",
        "def parse_sentence(sentence):\n",
        "    sentence = sentence.split()\n",
        "    if parse(sentence, 0, 'S') == len(sentence):\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Example usage\n",
        "sentence = \"the student reads a book\"\n",
        "if parse_sentence(sentence):\n",
        "    print(\"Parsing successful!\")\n",
        "else:\n",
        "    print(\"Parsing failed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3VpocYQXtjj",
        "outputId": "058f578e-d4b8-4fe0-c3f5-51729d903d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IN-LAB-2\n"
      ],
      "metadata": {
        "id": "Nvce108HZt28"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the grammar rules\n",
        "grammar = {\n",
        "    'S': [['NP', 'VP']],\n",
        "    'NP': [['the', 'noun'], ['a', 'noun']],\n",
        "    'VP': [['verb', 'NP']],\n",
        "    'noun': ['telescope', 'book', 'student'],\n",
        "    'verb': ['reads', 'studies']\n",
        "}\n",
        "\n",
        "# Function to perform shift operation\n",
        "def shift(stack, buffer):\n",
        "    stack.append(buffer.pop(0))\n",
        "    return stack, buffer\n",
        "\n",
        "# Function to perform reduce operation\n",
        "def reduce(stack):\n",
        "    for lhs, rhs in grammar.items():\n",
        "        rhs_len = len(rhs[0])\n",
        "        if stack[-rhs_len:] == rhs[0]:\n",
        "            stack = stack[:-rhs_len]\n",
        "            stack.append(lhs)\n",
        "            return stack\n",
        "    return stack\n",
        "\n",
        "# Main parsing function\n",
        "def parse(sentence):\n",
        "    sentence = sentence.split()\n",
        "    stack = []\n",
        "    buffer = sentence.copy()\n",
        "\n",
        "    while len(buffer) > 0:\n",
        "        stack, buffer = shift(stack, buffer)\n",
        "        stack = reduce(stack)\n",
        "\n",
        "    if len(stack) == 1 and stack[0] == 'S':\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "# Example usage\n",
        "sentence = \"the student reads a book\"\n",
        "if parse(sentence):\n",
        "    print(\"Parsing successful!\")\n",
        "else:\n",
        "    print(\"Parsing failed!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vvemn0xMZwSK",
        "outputId": "379af86b-2d21-4957-f2f8-c0084dc9ae0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parsing failed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "POST-LAB"
      ],
      "metadata": {
        "id": "I_qIuOiEb9pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recursive Descent Parsing\n",
        "def recursive_descent_parse(input_str, grammar, index=0, symbol='S'):\n",
        "    if index == len(input_str): return True\n",
        "    if symbol in grammar:\n",
        "        for prod in grammar[symbol]:\n",
        "            i = index\n",
        "            for part in prod:\n",
        "                if part in grammar:\n",
        "                    if not recursive_descent_parse(input_str, grammar, i, part): break\n",
        "                elif i < len(input_str) and input_str[i] == part: i += 1\n",
        "                else: break\n",
        "            else: return True\n",
        "    return False\n",
        "\n",
        "# Shift-Reduce Parsing\n",
        "def shift_reduce_parse(input_str, grammar):\n",
        "    stack, tokens = [], input_str.split()\n",
        "    while tokens:\n",
        "        stack.append(tokens.pop(0))\n",
        "        while len(stack) > 1:\n",
        "            for lhs, rhs_list in grammar.items():\n",
        "                for rhs in rhs_list:\n",
        "                    if stack[-len(rhs):] == rhs:\n",
        "                        stack[-len(rhs):] = [lhs]\n",
        "                        break\n",
        "                else: continue\n",
        "                break\n",
        "            else: break\n",
        "    return len(stack) == 1 and stack[0] == 'S'\n",
        "\n",
        "# Example usage\n",
        "input_str = \"( id + id ) * id\"\n",
        "print(\"Recursive Descent Parsing:\", recursive_descent_parse(input_str.split(), recursive_descent_grammar))\n",
        "print(\"Shift-Reduce Parsing:\", shift_reduce_parse(input_str, shift_reduce_grammar))\n",
        "\n"
      ],
      "metadata": {
        "id": "vuV2-2encAE_",
        "outputId": "1f85d1ec-b4a6-4d07-d2e3-9b86e1e933b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'recursive_descent_grammar' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-783835a441da>\u001b[0m in \u001b[0;36m<cell line: 33>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Example usage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0minput_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"( id + id ) * id\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Recursive Descent Parsing:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive_descent_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursive_descent_grammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shift-Reduce Parsing:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_reduce_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshift_reduce_grammar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'recursive_descent_grammar' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In LAB 11"
      ],
      "metadata": {
        "id": "GHkUfZuSDdii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download necessary resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "# Define the sentence to parse\n",
        "sentence = \"I saw a man with a telescope\"\n",
        "# Tokenize the sentence\n",
        "tokens = nltk.word_tokenize(sentence)\n",
        "# Define the grammar\n",
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        " S -> NP VP\n",
        " NP -> Det N | Det N PP | 'I'\n",
        " VP -> V | V NP | V NP PP\n",
        " PP -> P NP\n",
        " Det -> 'a' | 'an' | 'the'\n",
        " N -> 'man' | 'telescope' | 'park'\n",
        " V -> 'saw' | 'ate' | 'walked'\n",
        " P -> 'in' | 'with'\n",
        "\"\"\")\n",
        "# Create and use the Earley parser\n",
        "parser = nltk.EarleyChartParser(grammar)\n",
        "parses = parser.parse(tokens)\n",
        "# Print the parse trees\n",
        "for tree in parses:\n",
        " print(tree)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tn6XRwChDcWR",
        "outputId": "aac5c380-8fdc-4d76-8153-4158699126ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP (Det a) (N man))\n",
            "    (PP (P with) (NP (Det a) (N telescope)))))\n",
            "(S\n",
            "  (NP I)\n",
            "  (VP\n",
            "    (V saw)\n",
            "    (NP (Det a) (N man) (PP (P with) (NP (Det a) (N telescope))))))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "IN-LAB2"
      ],
      "metadata": {
        "id": "ZXGggV6SSTnt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lmLWJCRGSYkq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.classify.util import accuracy as nltk_accuracy\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('movie_reviews')\n",
        "\n",
        "# Function to extract features from text\n",
        "def extract_features(words):\n",
        "    return dict([(word, True) for word in words])\n",
        "\n",
        "# Load movie reviews dataset from NLTK\n",
        "reviews = [(list(movie_reviews.words(fileid)), category)\n",
        "           for category in movie_reviews.categories()\n",
        "           for fileid in movie_reviews.fileids(category)]\n",
        "\n",
        "# Shuffle the reviews\n",
        "import random\n",
        "random.shuffle(reviews)\n",
        "\n",
        "# Extract features from the reviews\n",
        "featuresets = [(extract_features(words), category) for (words, category) in reviews]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_size = int(0.8 * len(featuresets))\n",
        "train_set, test_set = featuresets[:train_size], featuresets[train_size:]\n",
        "\n",
        "# Train a Naive Bayes classifier\n",
        "classifier = NaiveBayesClassifier.train(train_set)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = nltk_accuracy(classifier, test_set)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Test the classifier with custom input\n",
        "custom_review = \"This movie was great!\"\n",
        "custom_features = extract_features(custom_review.split())\n",
        "print(\"Sentiment:\", classifier.classify(custom_features))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POGNJU_E_H4G",
        "outputId": "ab28c6b2-d597-4283-c95d-ad47c4879b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package movie_reviews to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/movie_reviews.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7425\n",
            "Sentiment: neg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lh9NauiNek87"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkLIuUaMXk7gMMY6uRGRii",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}